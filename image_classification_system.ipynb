{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:14:36.882945500Z",
     "start_time": "2024-02-16T16:14:34.982006500Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F  # a module in PyTorch that contains functions like activation functions, loss functions, and more.\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#  this line of code sets up a pipeline that first converts images to the tensor format suitable for neural networks and then normalizes their pixel values to help with the efficient training of the network.\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))] )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:14:36.890967200Z",
     "start_time": "2024-02-16T16:14:36.882945500Z"
    }
   },
   "id": "1fc05a78ebac9254",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:14:36.898016300Z",
     "start_time": "2024-02-16T16:14:36.889904400Z"
    }
   },
   "id": "d08f528369f0cd84",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10 dataset contains 60,000 color images of 32x32 pixels, evenly divided into 10 classes (like cars, birds, cats, etc.), with 6,000 images per class. The dataset is split into 50,000 training images and 10,000 test images.\n",
    "\n",
    "# Load the training portion of the CIFAR-10 dataset, download it to the specified folder if it's not there already, and apply the specified preprocessing steps to each image in the dataset.\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:14:37.584286200Z",
     "start_time": "2024-02-16T16:14:36.896982700Z"
    }
   },
   "id": "f2b50040163648b4",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#  this line of code sets up an efficient way to iterate through the training data, providing batches of four images at a time, shuffled for each epoch to improve learning, and using two parallel workers to ensure that data is always ready for the model to train on, minimizing downtime. \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:14:37.592460300Z",
     "start_time": "2024-02-16T16:14:37.585973Z"
    }
   },
   "id": "20f9f73ee02c2441",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# for the test set:\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:14:38.159201400Z",
     "start_time": "2024-02-16T16:14:37.590208600Z"
    }
   },
   "id": "8643b3a19568aabb",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# The Net class represents a specific architecture for a Convolutional Neural Network (CNN).\n",
    "# 3 channels for RGB color images.\n",
    "# 6 filters apply, 6 different feature maps, where each feature map is essentially a 2D array that represents some features detected in the input.\n",
    "# 5 the size of the kernel.\n",
    "# The second convolutional layer takes the 6 channels produced by conv1 as input and produces 16 output channels, also with a 5x5 kernel.\n",
    "# A max pooling layer with a 2x2 window and stride of 2. \n",
    "# fc1, fc2, fc3: \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)  # The output of the last pooling layer is flattened to transform it into a vector suitable for input into the fully connected layers.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:14:38.164010500Z",
     "start_time": "2024-02-16T16:14:38.162997600Z"
    }
   },
   "id": "1a0fef479667533d",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "net = Net()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:14:38.172555600Z",
     "start_time": "2024-02-16T16:14:38.165046Z"
    }
   },
   "id": "c8311b776c34886c",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    " # Choose a loss function and an optimizer.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "epochs = 20  # An epoch represents one complete pass through the entire training dataset. "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:14:38.176621900Z",
     "start_time": "2024-02-16T16:14:38.172555600Z"
    }
   },
   "id": "97c989ca787390f8",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.6788742968332768\n",
      "Epoch 2, Loss: 1.3135503541347384\n",
      "Epoch 3, Loss: 1.1683994162085651\n",
      "Epoch 4, Loss: 1.077977432640791\n",
      "Epoch 5, Loss: 1.0103684315036237\n",
      "Epoch 6, Loss: 0.9564652267588303\n",
      "Epoch 7, Loss: 0.9114224910722672\n",
      "Epoch 8, Loss: 0.8774170165339019\n",
      "Epoch 9, Loss: 0.8432624717425881\n",
      "Epoch 10, Loss: 0.8159883855467476\n",
      "Epoch 11, Loss: 0.7885373961635446\n",
      "Epoch 12, Loss: 0.763369266916823\n",
      "Epoch 13, Loss: 0.7506062280585477\n",
      "Epoch 14, Loss: 0.7266837144618249\n",
      "Epoch 15, Loss: 0.7097512524378119\n",
      "Epoch 16, Loss: 0.6937284594947618\n",
      "Epoch 17, Loss: 0.6778485400003169\n",
      "Epoch 18, Loss: 0.667255125693793\n",
      "Epoch 19, Loss: 0.6594374755301181\n",
      "Epoch 20, Loss: 0.6538797184945114\n"
     ]
    }
   ],
   "source": [
    "epoch_losses = []  # List to store loss of each epoch\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0  # At the start of each epoch, the running loss is reset to 0. This variable will accumulate the loss from each batch in the dataset, allowing you to calculate the average loss for the epoch.\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        # Before calculating the new gradients, existing gradients are reset to zero. Gradients accumulate by default, for correct parameter updates during backpropagation, but we need to clear them before computing gradients for the next batch.\n",
    "        optimizer.zero_grad()  # zero the parameter gradients\n",
    "        # The model (net) processes the inputs through its forward method to produce predictions (outputs)\n",
    "        outputs = net(inputs)\n",
    "        # The loss function (criterion), which measures how well the model's predictions match the actual labels, is applied to compute the loss for the current batch.\n",
    "        loss = criterion(outputs, labels)\n",
    "        #  calculates the gradient of the loss function with respect to the model parameters by backpropagation. These gradients are used to adjust the model's parameters in the direction that minimally reduces the loss.\n",
    "        loss.backward()\n",
    "        # This applies the gradients computed by loss.backward() to update the model's parameters. The optimizer is responsible for adjusting the parameters based on the gradients to minimize the loss function.\n",
    "        optimizer.step()\n",
    "        # Adds the loss of the current batch to the running loss.\n",
    "        running_loss += loss.item()\n",
    "    # After processing all batches in the dataset for the epoch, the average loss is printed.\n",
    "    epoch_loss = running_loss / len(trainloader)\n",
    "    epoch_losses.append(epoch_loss)  # Append the average loss for this epoch to the list\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss}\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:28:21.786604800Z",
     "start_time": "2024-02-16T16:14:38.175578Z"
    }
   },
   "id": "baccda0bed88cdb3",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[1.6788742968332768,\n 1.3135503541347384,\n 1.1683994162085651,\n 1.077977432640791,\n 1.0103684315036237,\n 0.9564652267588303,\n 0.9114224910722672,\n 0.8774170165339019,\n 0.8432624717425881,\n 0.8159883855467476,\n 0.7885373961635446,\n 0.763369266916823,\n 0.7506062280585477,\n 0.7266837144618249,\n 0.7097512524378119,\n 0.6937284594947618,\n 0.6778485400003169,\n 0.667255125693793,\n 0.6594374755301181,\n 0.6538797184945114]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_losses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:28:21.793333100Z",
     "start_time": "2024-02-16T16:28:21.783382300Z"
    }
   },
   "id": "85c8bc35c3ed7e52",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Plotting the loss\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(range(1, epochs+1), epoch_losses, marker='o', linestyle='-', color='blue')\n",
    "# plt.title('Epoch vs Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:28:21.794441900Z",
     "start_time": "2024-02-16T16:28:21.790243Z"
    }
   },
   "id": "324b7ea8c3ce5cdd",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "correct = 0  # This initializes a counter for the number of predictions that the model gets right.\n",
    "total = 0  # This initializes a counter for the total number of predictions made (or the total number of labels in the test dataset).\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:28:21.801086400Z",
     "start_time": "2024-02-16T16:28:21.794386700Z"
    }
   },
   "id": "2ddf92e4df8989b3",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 61.47%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():  # we do not need to calculate gradients during this operation. Since we're only evaluating the model and not training it, we don't need gradients, which saves memory and computation.\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)  # This adds the number of labels in the current batch to the total count of labels. It effectively counts the number of test images processed so far.\n",
    "        correct += (predicted == labels).sum().item()  # This calculates the number of correct predictions in the current batch by comparing predicted with labels. If a prediction matches the true label, it's counted as correct. .sum().item() adds up all the correct predictions to get a single number.\n",
    "\n",
    "print(f\"Accuracy on the test set: {(100 * correct / total):.2f}%\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:28:28.019587Z",
     "start_time": "2024-02-16T16:28:21.801086400Z"
    }
   },
   "id": "7c43514801946b7e",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Save\n",
    "#  This line defines a variable PATH that stores the file path where the model's state dictionary should be saved. The file extension .pth is commonly used for PyTorch model files, but it's not a requirement; you could use another extension if you prefer. \n",
    "PATH = './model/cifar_net.pth'\n",
    "\n",
    "# net.state_dict(): This method returns the model's state dictionary. The state dictionary is a Python dictionary object that maps each layer to its parameter tensor. Notably, the state dictionary contains weights and biases of the model layers, but it does not contain the model architecture itself. This means that when you load the model from this file, you'll need to have the model class (Net in your case) defined in your code.\n",
    "torch.save(net.state_dict(), PATH)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:28:28.020847400Z",
     "start_time": "2024-02-16T16:28:28.014876100Z"
    }
   },
   "id": "2b917f045ac1e1af",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:28:28.029813600Z",
     "start_time": "2024-02-16T16:28:28.019587Z"
    }
   },
   "id": "dfca62ae6470cdc6",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "net.eval()\n",
    "# Define the transformation\n",
    "transform_im = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize the image to 32x32 pixels\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Assuming these were the normalization values used during training\n",
    "])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:28:28.031894600Z",
     "start_time": "2024-02-16T16:28:28.027883Z"
    }
   },
   "id": "d7249300b50c4847",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load and transform the image\n",
    "image_path = './imgs_input/bird.jpg'  # Update this to the path of your image\n",
    "image = Image.open(image_path)\n",
    "image = transform_im(image)\n",
    "image = image.unsqueeze(0)  # Add a batch dimension\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:28:28.071988500Z",
     "start_time": "2024-02-16T16:28:28.031894600Z"
    }
   },
   "id": "a1e81734da4dcc12",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Predict the class\n",
    "outputs = net(image)\n",
    "_, predicted = torch.max(outputs, 1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:28:28.073090400Z",
     "start_time": "2024-02-16T16:28:28.056097700Z"
    }
   },
   "id": "787b259d0e1e97b5",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cifar10_classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:28:28.073090400Z",
     "start_time": "2024-02-16T16:28:28.061898100Z"
    }
   },
   "id": "60817d7479afde64",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: bird\n"
     ]
    }
   ],
   "source": [
    "print(f'Predicted class: {cifar10_classes[predicted[0]]}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:28:28.074214800Z",
     "start_time": "2024-02-16T16:28:28.065567600Z"
    }
   },
   "id": "9a25dde9d20395c7",
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
